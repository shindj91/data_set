# -*- coding: utf-8 -*-
"""ML_segr_re.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAJUi-u5SV5jSsOpaoZaWSACnOcNXcIq

#**1. Data loading and a primitive test**
"""

!python

from google.colab import drive
drive.mount('/content/drive') # Ctrl+Alt+D

from pandas import read_csv, DataFrame
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential, optimizers
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import normalize
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# load the dataset
path = 'drive/My Drive/CCEL/machine_learning/data_segr_2.csv'
df = read_csv(path, header=0)

# split into input and output columns
X, y = df.values[:,:-1], df.values[:,-1]
# X = normalize(X.astype('float32')) # input normalization
X = (X-np.mean(X)) / np.std(X) # input standardization

# split into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# # determine the number of input features
# n_features = X_train.shape[1]


# # define model
# model = Sequential()
# model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))
# # model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))
# # model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))
# model.add(Dense(1))
# # compile the model
# optimizer = optimizers.Adam()
# model.compile(optimizer=optimizer, loss='mse', metrics=['mae',])

# # fit the model
# callback = EarlyStopping(monitor='val_loss',
#                          patience=30,
#                          verbose=1,
#                          restore_best_weights=True)
# # This callback will stop the training when there is no improvement in
# # the validation loss for three consecutive epochs.

# history = model.fit(X_train, y_train,
#                     validation_data=(X_test, y_test),
#                     epochs=200, batch_size=202, callbacks=[callback],
#                     verbose=1)

# # evaluate the model
# loss, acc = model.evaluate(X_test, y_test, verbose=0)
# print('Test Accuracy, MAE(eV)={:8.5f}, MSE={:8.5f}'.format(acc, loss))

# # plot learning curves
# plt.title('Learning Curves')
# plt.xlabel('Epoch')
# plt.ylabel('MSE (eV)')
# plt.plot(history.history['loss'], label='train')
# plt.plot(history.history['val_loss'], label='val')
# plt.legend()
# plt.show()

# # make a prediction
# yhat= model.predict(X)
# #print(yhat)
# plt.title('Parity plot for segregation energy')
# plt.xlabel('y (eV)')
# plt.ylabel('y_predicted (eV)')
# plt.scatter(y, yhat)
# bottom, top = plt.xlim()
# plt.ylim(bottom, top)
# plt.show()

def test_params(X_train, y_train, X_test, y_test,
                structure, learning_rate=0.01,
                kernel_regularizer=None,
                activation='relu', optimizer='adam',
                patience=30, epochs=500, batch_size=32,
                visual=False, verbose=0,
                fig_path=None):
  
  # determine the number of input features
  n_features = X_train.shape[1]
  # define model
  model = Sequential()
  for i in range(len(structure)):
    if i == 0:
      model.add(Dense(structure[i], activation=activation, kernel_initializer='he_normal',
                      kernel_regularizer=kernel_regularizer,
                      input_shape=(n_features,)))
    else:
      model.add(Dense(structure[i], activation=activation, kernel_initializer='he_normal',
                      kernel_regularizer=kernel_regularizer))
  model.add(Dense(1)) # linear activation
  # compile the model
  model.compile(optimizer=optimizer, loss='mse', metrics=['mae',])

  # fit the model
  callback = EarlyStopping(monitor='val_loss',
                          patience=patience,
                          verbose=verbose,
                          restore_best_weights=True)
  # This callback will stop the training when there is no improvement in
  # the validation loss for three consecutive epochs.

  history = model.fit(X_train, y_train,
                      validation_data=(X_test, y_test),
                      epochs=epochs, batch_size=batch_size, callbacks=[callback],
                      verbose=verbose)

  # evaluate the model
  loss_train, acc_train = model.evaluate(X_train, y_train, verbose=0)
  loss_test, acc_test = model.evaluate(X_test, y_test, verbose=0)
  # print('MAE_train(eV) = {:8.5f} MSE_train = {:8.5f} MAE_test(eV) = {:8.5f}, MSE_test(eV) = {:8.5f}'
  #       .format(acc_train, loss_train, acc_test, loss_test))

  if verbose:
    # model summary
    model.summary()

  if visual:
    fig = plt.figure(figsize=(8,12))
    # plot learning curves
    plt1 = fig.add_subplot(211)
    plt1.set_title('Learning Curves')
    plt1.set_xlabel('Epoch')
    plt1.set_ylabel('MSE (eV)')
    plt1.plot(history.history['loss'], label='train')
    plt1.plot(history.history['val_loss'], label='val')
    plt1.legend()

    # make a prediction
    yhat= model.predict(X)
    plt2 = fig.add_subplot(212)
    plt2.set_title('Parity plot for segregation energy')
    plt2.set_xlabel('y (eV)')
    plt2.set_ylabel('y_predicted (eV)')
    plt2.scatter(y, yhat)
    bottom, top = plt2.get_xlim()
    plt2.set_ylim(bottom, top)
    plt2.text(0.0,-1.0,'{:1.4f}, {:1.4f}, {:1.4f}, {:1.4f}'.format(loss_train, acc_train, loss_test, acc_test), size=10)
    fig.savefig(fig_path, dpi=300, format='png',)
    # fig.show()

  return loss_train, acc_train, loss_test, acc_test, model

"""#**2.1. Checking if the function works well**"""

import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    test_params(X_train, y_train, X_test, y_test,
                structure=[5, 2],
                epochs=500, batch_size=202, visual=False, verbose=0)
    # return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    test_params(X_train, y_train, X_test, y_test,
                structure=[5, 2],
                epochs=500, batch_size=202, visual=False, verbose=0)
    # return tf.math.reduce_sum(net_gpu)
  
# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
print('Warming up ...')
cpu()
gpu()

# Run the op several times.
# print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
#       '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=5, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=5, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

"""#**3. Hyperparameter tuning**

> a) Optimizer

> b) Learning rate

> c) batch size (can be omitted thanks to small size of data set)

> d) Input noise (maybe later ...)

> e) Network design -number of hidden layers and number of neurons

> f) Regularizers - (L1, L2, dropout etc.)

> But, again, every dataset is different and hyper-parameters will surely be dependent on that. So, for every problem one approach won't do. Plotting the error will give the feel for the dataset and help in finding the 'optimal' hyper-parameters.

**Reference**
##### naive (https://stats.stackexchange.com/users/168306/naive), What is a sensible order for parameter tuning in neural networks?, URL (version: 2018-09-29): https://stats.stackexchange.com/q/369315

#**3.1. Optimizer tuning**
"""

optimizers = ['Adadelta', 'Adagrad', 'Adam', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop', 'SGD']
for optimizer in optimizers:
  print("testing {0} ...".format(optimizer), end=' ')
  test_params(X_train, y_train, X_test, y_test,
              structure=[5,], optimizer=optimizer, patience=30,
              epochs=500, batch_size=202, visual=False, verbose=0)

plt.figure(figsize=(10,8))
plt.xticks(fontsize=12)
plt.yticks(fontsize=14)
plt.bar(optimizers, [2.71274, 0.20813, 0.20764, 0.20776, 0.44872, 0.21262, 0.21229,0.20993],
        color = ['red', 'orange', 'pink', 'lightgreen', 'green', 'blue', 'lightblue', 'pink'])
plt.ylabel('MSE (eV)', fontsize=20)
plt.ylim(0.205, 0.216)
plt.show()

"""#**3.1.2 Applying K-fold Cross Validation**"""

from sklearn.model_selection import KFold
n_split = 5
trial = 5
optimizers = ['Adadelta', 'Adagrad', 'Adam', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop', 'SGD']
# optimizers = ['Adadelta']
for num in range(trial):
  print("{}th trial for stochasticity ...".format(num+1))
  for optimizer in optimizers:
    mse_avg, mae_avg = [],[] # for test data
    i = 0
    # print("testing {0} ...".format(optimizer), end=' ')
    print("testing {0} ...".format(optimizer))
    for train_index, test_index in KFold(n_splits = n_split, shuffle=True).split(X):
      i += 1 
      x_train,x_test=X[train_index], X[test_index]
      y_train,y_test=y[train_index], y[test_index]
      print("{}th fold ...".format(i), end=' ')
      # print("train_index: {}, test_index: {}".format(train_index, test_index))

      loss, acc, _ = test_params(x_train, y_train, x_test, y_test,
                              structure=[5,], optimizer=optimizer, patience=100,
                              epochs=1000, batch_size=128, visual=False, verbose=0)

      mse_avg.append(loss)
      mae_avg.append(acc)

    print(sum(mse_avg)/n_split, sum(mae_avg)/n_split)

# load the dataset
path_opt = 'drive/My Drive/CCEL/machine_learning/200818_optimizer_tuning.csv'
errors_optimizer = read_csv(path_opt, header=0)
errors_optimizer = DataFrame(errors_optimizer)

errors_optimizer

import seaborn as sns
sns.set(font_scale=1.5, font='sans-serif', palette=sns.color_palette('RdBu'))
errors_opt_mae = errors_optimizer.loc[errors_optimizer['error type']=='MAE']
# ax = sns.violinplot(x="optimizer", y="error value", hue="error type",
#                     data=errors_opt_mae, split=False, height=10, aspect=1.0)
ax = sns.catplot(x="optimizer", y="error value", col="error type", kind="violin",
                 height=10, aspect=1.0, col_wrap=10, hue="error type",
                 data=errors_optimizer, split=False)

ax.set_ylabels('CV_score')
ax.set_xlabels('Optimizer')

# ax.set_xlabel("Optimizer", fontsize=10)
# ax.tick_params(labelsize=5)

"""#**3.2. Learning Rate Optimization**"""

# load the dataset
path = 'drive/My Drive/CCEL/machine_learning/data_segr_2.csv'
df = read_csv(path, header=0)

# split into input and output columns
X, y = df.values[:,:-1], df.values[:,-1]
# X = normalize(X.astype('float32')) # input normalization
X = (X-np.mean(X)) / np.std(X) # input standardization

# split into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

from sklearn.model_selection import KFold

n_split = 5
trial = 5
learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]

for num in range(trial):
  print("{}th trial".format(num+1)) # several trials for stochasticity
  for learning_rate in learning_rates:
    mse_avg, mae_avg = [],[] # for test data
    i = 0
    print("testing learning rate={0} ...".format(learning_rate), end=' ')
    for train_index, test_index in KFold(n_splits = n_split, shuffle=True).split(X):
      i += 1 
      x_train,x_test=X[train_index], X[test_index]
      y_train,y_test=y[train_index], y[test_index]
      print("{}th fold ...".format(i), end=' ')
      # print("train_index: {}, test_index: {}".format(train_index, test_index))

      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
      loss, acc, _ = test_params(x_train, y_train, x_test, y_test,
                              structure=[5,], optimizer=optimizer, patience=100,
                              learning_rate=learning_rate,
                              epochs=1000, batch_size=128, visual=False, verbose=0)

      mse_avg.append(loss)
      mae_avg.append(acc)

    print(sum(mse_avg)/n_split, sum(mae_avg)/n_split)

# load the dataset
path_lr = 'drive/My Drive/CCEL/machine_learning/200822_learning_rate_tuning.csv'
errors_lr = read_csv(path_lr, header=0)
errors_lr = DataFrame(errors_lr)

errors_lr.tail()

sns.set(font_scale=1.5, font='sans-serif')#, palette=sns.color_palette('Pastel1'))
# errors_opt_mae = errors_optimizer.loc[errors_optimizer['error type']=='MAE']
# ax = sns.violinplot(x="optimizer", y="error value", hue="error type",
#                     data=errors_opt_mae, split=False, height=10, aspect=1.0)
ax = sns.catplot(x="learning rate", y="cv_score", col="score_type", kind="violin",
                 height=10, aspect=1.0, hue="score_type",
                 data=errors_lr, split=False)

plt.ylim(0.15, 0.45)
ax.set_ylabels('CV_score')
ax.set_xlabels('Learning Rate')

# ax.set_xlabel("Optimizer", fontsize=10)
# ax.tick_params(labelsize=5)

"""#**3.2. Neural Net Structure Optimization**"""

structures = [[i] for i in range(1,11,1)]; 
print(structures)
structures = [[i,j] for i in range(1,11) for j in range(1,11)]
print(structures)
structures = [[i,j,k] for i in range(1,11) for j in range(1,11) for k in range(1,11)]
print(structures, len(structures))

from sklearn.model_selection import KFold

n_split = 5
trial = 5
learning_rates = 0.1
# structures = [[i,j,k] for i in range(1,11) for j in range(1,11) for k in range(1,11)]
structures = [[i] for i in range(1,11,1)]

print("trial,structure,cv_score,score_type,optimizer ")
for num in range(trial):
  # print("{}th trial".format(num+1)) # several trials for stochasticity
  for structure in structures:
    mse_avg, mae_avg = [],[] # for test data
    i = 0
    for train_index, test_index in KFold(n_splits = n_split, shuffle=True).split(X):
      i += 1 
      x_train,x_test=X[train_index], X[test_index]
      y_train,y_test=y[train_index], y[test_index]
      # print("...".format(i), end=' ') # fold
      # print("train_index: {}, test_index: {}".format(train_index, test_index))

      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
      loss, acc, _ = test_params(x_train, y_train, x_test, y_test,
                              structure=structure, optimizer=optimizer, patience=100,
                              learning_rate=learning_rate,
                              epochs=1000, batch_size=128, visual=False, verbose=0)

      mse_avg.append(loss)
      mae_avg.append(acc)

    print('{},{},{},{},{}'.format(num+1, structure, sum(mse_avg)/n_split, 'MSE', 'adam'))
    print('{},{},{},{},{}'.format(num+1, structure, sum(mae_avg)/n_split, 'MAE', 'adam'))

# load the dataset
path_st = 'drive/My Drive/CCEL/machine_learning/200822_structure_tuning.csv'
errors_st = read_csv(path_st, header=0)
errors_st = DataFrame(errors_st)

errors_st.head()

sns.set(font_scale=1.5, font='sans-serif')#, palette=sns.color_palette('Pastel1'))
# errors_opt_mae = errors_optimizer.loc[errors_optimizer['error type']=='MAE']
# ax = sns.violinplot(x="optimizer", y="error value", hue="error type",
#                     data=errors_opt_mae, split=False, height=10, aspect=1.0)
ax = sns.catplot(x=" structure", y=" cv_score", col="score_type", kind="violin",
                 height=10, aspect=1.0, hue="score_type",
                 data=errors_st, split=False)

plt.ylim(0.32, 0.38)
ax.set_ylabels('CV_score')
ax.set_xlabels('Structure')

# ax.set_xlabel("Optimizer", fontsize=10)
# ax.tick_params(labelsize=5)

"""#3.2.1. NN optimization (updated, 200824)"""

structures = [[9,4], [10,4]] + [[i,j] for j in range(5,11) for i in range(1,11)]
structures

from sklearn.model_selection import KFold

n_split = 10
trial = 1
learning_rate = 0.003
# structures = [[i,j] for i in range(1,11) for j in range(1,11)]
# structures = [[9,8], [10,8]] + [[i,j] for j in range(9,11) for i in range(1,11)]
structures = [[15,30,25,15,5]]

print("trial,structure,cv_mse_train,cv_mae_train,cv_mse_test,cv_mae_test")
for num in range(trial):
  # print("{}th trial".format(num+1)) # several trials for stochasticity
  for structure in structures:
    mse_avg_train, mae_avg_train, mse_avg_test, mae_avg_test = [],[],[],[]
    i = 0
    for train_index, test_index in KFold(n_splits = n_split, shuffle=True).split(X):
      print('... ', end=' ')
      i += 1 
      x_train,x_test=X[train_index], X[test_index]
      y_train,y_test=y[train_index], y[test_index]
      # print("...".format(i), end=' ') # fold
      # print("train_index: {}, test_index: {}".format(train_index, test_index))

      model_path = '/content/drive/My Drive/CCEL/machine_learning/200824_structure_tuning/{}_{}_cv{}'.format(str(structure), learning_rate, i)
      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
      loss_train, acc_train, loss_test, acc_test, model = test_params(x_train, y_train, x_test, y_test,
                                                                       structure=structure, optimizer=optimizer, patience=1000,
                                                                       activation=tf.keras.layers.LeakyReLU(),
                                                                       kernel_regularizer=tf.keras.regularizers.l2(l2=0.002),
                                                                       learning_rate=learning_rate,
                                                                       epochs=20000, batch_size=302, visual=True, verbose=0,
                                                                       fig_path=model_path+'.png')

      model.save(model_path+'.h5')

      mse_avg_train.append(loss_train)
      mae_avg_train.append(acc_train)
      mse_avg_test.append(loss_test)
      mae_avg_test.append(acc_test)

    # print('MAE_train(eV) = {:8.5f} MSE_train = {:8.5f} MAE_test(eV) = {:8.5f}, MSE_test(eV) = {:8.5f}'
    #       .format(acc_train, loss_train, acc_test, loss_test
    print('{},{},{},{},{},{}'.format(num+1,
                                  structure, 
                                  sum(mse_avg_train)/n_split, 
                                  sum(mae_avg_train)/n_split,
                                  sum(mse_avg_test)/n_split, 
                                  sum(mae_avg_test)/n_split))

"""왜 structure에 따른 차이가 별로 없어 보이지? visual을 켜고 early_stopping 을 꺼보자.

#3.3. Regularizer Optimization (L2, $\lambda$ tuning)#
"""

from sklearn.model_selection import KFold

n_split = 5
trial = 5
learning_rates = 0.1
structure = ?
lambds = []

print("trial, structure, cv_score, cv_type, optimizer ")
for num in range(trial):
  # print("{}th trial".format(num+1)) # several trials for stochasticity
  for lambd in lambds:
    mse_avg, mae_avg = [],[] # for test data
    i = 0
    for train_index, test_index in KFold(n_splits = n_split, shuffle=True).split(X):
      i += 1 
      x_train,x_test=X[train_index], X[test_index]
      y_train,y_test=y[train_index], y[test_index]
      # print("...".format(i), end=' ') # fold
      # print("train_index: {}, test_index: {}".format(train_index, test_index))

      kernel_regularizer = tf.keras.regularizers.l2(l2=lambd)
      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
      loss, acc, _ = test_params(x_train, y_train, x_test, y_test,
                                 kernel_regularizer=kernel_regularizer,
                                 structure=structure, optimizer=optimizer, patience=100,
                                 learning_rate=learning_rate,
                                 epochs=1000, batch_size=128, visual=False, verbose=0)

      mse_avg.append(loss)
      mae_avg.append(acc)

    print('{}, {}, {}, {}, {}'.format(num+1, structure, sum(mse_avg)/n_split, 'MSE', 'adam'))
    print('{}, {}, {}, {}, {}'.format(num+1, structure, sum(mae_avg)/n_split, 'MAE', 'adam'))

"""#regular test (baby-sitting)"""

# load the dataset
path = 'drive/My Drive/CCEL/machine_learning/data_segr_2.csv'
df = read_csv(path, header=0)

# split into input and output columns
X, y = df.values[:,:-1], df.values[:,-1]
# X = normalize(X.astype('float32')) # input normalization
X = (X-np.mean(X)) / np.std(X) # input standardization

# split into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

# structures = [[i, j, k] for i in range(10,12) for j in range(10,15) for k in range(10,15)]
# structures = [[i, j] for i in range(1,11) for j in range(1,11)]
structures = [[11, 14, 11, 5]]
# for lr in [0.002, 0.003, 0.004, 0.005]:
for structure in structures:
  lr = 0.003
  optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
  # lambd = 0.1
  # kernel_regularizer = tf.keras.regularizers.l2(l2=lambd)

  model_path =  '/content/drive/My Drive/CCEL/machine_learning/200822_structure_tuning/{}_{}_long_long'.format(str(structure), lr)

  print('training structure: {} ...'.format(str(structure)))
  _, _, _, _, model = test_params(X_train, y_train, X_test, y_test,
                            structure=structure, optimizer=optimizer, patience=1000,
                            activation=tf.keras.layers.LeakyReLU(),
                            kernel_regularizer=tf.keras.regularizers.l2(l2=0.002),
                            epochs=100, batch_size=202, visual=True, verbose=1,
                            fig_path=model_path + '.png')

  model.save(model_path + '.h5')

"""baby sitting으로 확인해보니 sigmoid가 잘 작동함. activation function으로도 hyperparameter tuning 필요 (200822)

# Load Models and Evaluate
"""

from glob import glob
# furthers = [[11, 10, 11], [11, 14, 11], [11, 14, 14], [13, 10, 14]]
# furthers = [[30,15,7,3]]
furthers = [[15,30,25,15,5]]
model_path = '/content/drive/My Drive/CCEL/machine_learning/200824_structure_tuning/'
model_paths = glob(model_path+'*.h5')
print(model_paths)

# for path in model_paths:
for further in furthers:
  model = tf.keras.models.load_model(model_path+str(further)+'_0.003_cv2.h5', 
                                     custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU})
  # evaluate the model
  mse_test, mae_test = model.evaluate(X_test, y_test, verbose=0)
  mse, mae = model.evaluate(X, y, verbose=0)
  print(model_path+str(further)+', {}, {}, {}, {}'.format(mae_test, mse_test, mae, mse))

# fig_path = '/content/drive/My Drive/CCEL/machine_learning/200822_structure_tuning/[11, 14, 11]_0.003_parity.png'

yhat= model.predict(X)
fig = plt.figure(figsize=(8,8))
plt2 = fig.add_subplot(111)
plt2.set_title('Parity plot for segregation energy', fontsize=20, pad=15)
plt2.set_xlabel('$E_{segr}$ (eV)', fontsize=20)
plt2.set_ylabel('$E_{segr,predicted}$ (eV)', fontsize=20)
plt2.scatter(y, yhat)
plt.plot([-2,2],[-2,2], c='k')
# plt.text(1.0,-1.5,'MAE=0.16 eV', size=10)
bottom, top = plt2.get_xlim()
plt2.set_ylim(bottom, top)
fig.show()
# fig.savefig(fig_path, dpi=300, format='png')

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

path = 'drive/My Drive/CCEL/machine_learning/dcenter_shift.csv'
df = read_csv(path, header=0)
elements = list(df.columns[1:])
elements

radius = {
            'Ti':3.05,
            'V': 2.82,
            'Cr':2.68,
            'Mn':2.70,
            'Fe':2.66,
            'Co':2.62,
            'Ni':2.60,
            'Cu':2.67,
            'Zr':3.35,
            'Nb':3.07,
            'Mo':2.99,
            'Tc':2.84,
            'Ru':2.79,
            'Rh':2.81,
            'Pd':2.87,
            'Ag':3.01,
            'Hf':3.30,
            'Ta':3.07,
            'W': 2.95,
            'Re':2.87,
            'Os':2.83,
            'Ir':2.84,
            'Pt':2.90,
            'Au':3.00 }

tension = {
            'Ti':1.650,
            'V': 1.950,
            'Cr':1.590,
            'Mn':1.060,
            'Fe':1.880,
            'Co':1.880,
            'Ni':1.780,
            'Cu':1.300,
            'Zr':1.480,
            'Nb':1.900,
            'Mo':2.250,
            'Tc':2.100,
            'Ru':2.250,
            'Rh':2.000,
            'Pd':1.500,
            'Ag':0.895,
            'Hf':1.630,
            'Ta':2.150,
            'W': 2.400,
            'Re':2.700,
            'Os':2.500,
            'Ir':2.250,
            'Pt':1.800,
            'Au':1.140 }

list_of_dict = [dict(zip(elements, [np.absolute(tension[element_host]-tension[element_impurity]) for element_host in elements])) for element_impurity in elements]
del_tension = dict(zip(elements, list_of_dict))

list_of_dict = [dict(zip(elements, [np.absolute(radius[element_host]-radius[element_impurity]) for element_host in elements])) for element_impurity in elements]
del_radius = dict(zip(elements, list_of_dict))

list_of_dict = [dict(zip(elements, [df[element][i] for i in range(24)])) for element in elements]
del_dcenter = dict(zip(elements,list_of_dict))

DataFrame(del_dcenter)

del_dcenter['V']['Co']

filename = '/content/drive/My Drive/CCEL/machine_learning/200824_structure_tuning/[30, 15, 7, 3]_0.003_cv2.h5'

columns_list = list(df.columns)
columns_list

def Esegr_direct(model_path, input):
  # load the model
  loaded_model = tf.keras.models.load_model(model_path, 
                                     custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU})

  # load the dataset
  path = 'drive/My Drive/CCEL/machine_learning/data_segr_2.csv'
  df = read_csv(path, header=0)

  # split into input and output columns
  X = df.values[:,:-1]
  mean = np.mean(X)
  std = np.std(X) # input standardization

  input = np.array(input).reshape(1,-1)
  input_norm = (input-mean) / std # input standardization

  Esegr = loaded_model.predict(input_norm)
  return float(Esegr)

columns_list

host_facet = 643
# element_host = 'Cu'
# element_impurity = 'Ag'

Esegr = DataFrame(
                  [dict(zip(columns_list+['element_host','element_impurity','host_facet'],
                            [6, 4, 3, 6, del_tension[element_host][element_impurity], del_radius[element_host][element_impurity], del_dcenter[element_host][element_impurity], 
                            Esegr_direct(filename, [6, 4, 3, 6, del_tension[element_host][element_impurity], del_radius[element_host][element_impurity], del_dcenter[element_host][element_impurity]]), 
                            element_host, element_impurity, host_facet])) for element_impurity in elements for element_host in elements]
                  )

Esegr_direct(filename,[1,1,1,6,0,0,0])

sns.set(font_scale=0.95, font='sans-serif')#, palette=sns.color_palette('Pastel1'))
# errors_opt_mae = errors_optimizer.loc[errors_optimizer['error type']=='MAE']

Esegr_pivot = Esegr.pivot("element_host","element_impurity","Esegr",)

"""## **In periodic order**"""

filename = '/content/drive/My Drive/CCEL/machine_learning/200827_SVR/{}_{}_cv{}.sav'.format(5000, 0.036000000000000004, 2)

list_of_dict = [dict(zip(elements, [Esegr_direct(filename, [6, 4, 3, 6, del_tension[element_host][element_impurity], del_radius[element_host][element_impurity], del_dcenter[element_host][element_impurity]]) for element_impurity in elements])) for element_host in elements]
Esegr_pair = dict(zip(elements, list_of_dict))

list_of_dict2 = [dict(zip(elements, [0 for element_impurity in elements])) for element_host in elements]
Esegr_mask = dict(zip(elements, list_of_dict2))

for element1 in elements:
  for element2 in elements:
    if element1 is element2:
      Esegr_mask[element1][element2] = True
    else:
      Esegr_mask[element1][element2] = False 

DataFrame(Esegr_mask).head()

DataFrame(Esegr_pair).head()

sns.set(font_scale=1.50, font='sans-serif',)#, palette=sns.color_palette('Pastel1'))
# Draw a heatmap with the numeric values in each cell
f, ax = plt.subplots(figsize=(24, 20))
heatmap = sns.heatmap(DataFrame(Esegr_pair), annot=False, fmt="1.2f",
                      linewidths=.5, ax=ax, cmap='RdBu',square=True,
                      mask=DataFrame(Esegr_mask),
                      vmin=-1.2, vmax=1.2, cbar_kws={'label': '$E_{segr}$ (eV)', 'pad': 0.02})

heatmap.set_xlabel('Impurity metals')
heatmap.set_ylabel('Host metals')
heatmap.set_facecolor('black')

"""Esegr_direct 함수상에서는 제대로 나오는 것 같은데 여기서는 부호가 반대인듯 함. -> host, impurity index가 반대로 되어 있었음."""

Esegr